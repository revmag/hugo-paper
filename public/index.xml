<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shivam Gupta</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Shivam Gupta</description>
    <generator>Hugo</generator>
    <language>en</language>
    <copyright>Â© Shivam Gupta</copyright>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>NMT Transformers from scratch</title>
      <link>http://localhost:1313/projects/a--transformers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/projects/a--transformers/</guid>
      <description>Built an encoder decoder tranformers for English to French experimenting with different attention heads</description>
    </item>
    <item>
      <title>Notes on Reading</title>
      <link>http://localhost:1313/misc/notesonreading/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/misc/notesonreading/</guid>
      <description>&lt;p&gt;&lt;strong&gt;The world is a very malleable place. If you know what you want, and you go for it with maximum energy and drive and passion, the world will often reconfigure itself around you much more quickly and easily than you would think.&#xA;~ Marc Andreessen&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;( The funny thing is either you believe it fully or you donâ€™t, there is not in between)&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;&lt;code&gt;On life:&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;Start by reading-&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Replacing Guilt&#xA;Can relate on a day to day level, with how to overcome guilt and do something&#xA;( state of motion is the best thing you can get out of this book)&lt;/p&gt;</description>
    </item>
    <item>
      <title>State of Mechanistic Interpretability</title>
      <link>http://localhost:1313/posts/stateofinterpretability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/stateofinterpretability/</guid>
      <description>&lt;p&gt;So I deep dived into what is going in the field of mechanistic interpretability( it seems really cool, to be able to know how attention is calculating all these amazing similarities from so far, and how it is able to make so much sense).&lt;/p&gt;&#xA;&lt;p&gt;What I think is happening -&#xA;It seems to me, LLMs mimic a lazy person, who is very good at pattern patching, so it learns rules/pattern of some kind implicitly, and its not entirely perfect knowledge like code, but it wants to converge to that. I got this idea by looking at the chess paper, where they analyzed how it predicts the next move of chess game, without being fed the rules explicitly, or what each piece does, but after training for several games, it was able to predict the next move, and also tell some of the rules, Some for calculation, there is this paper, which analyzed how LLMs are able to do 5-6 digit addition, it was amazing, it tried to show they are learning the rules of addition somehow( like addition at ones place, and then carry on if sum exceeds 10), by differentiating the dataset into different catagories (like whether carry sum is needed or not).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Grapes Farm data using S1 &amp; S2 satellite sensors</title>
      <link>http://localhost:1313/projects/b--farm-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/projects/b--farm-data/</guid>
      <description>&lt;p&gt;Analyzed grape farms using spectral angle mapping: first segmenting fields with SAM (Segment Anything), then computing the spectral angle of each pixel against known grape reference points to classify whether an area was a grape farm or not.&lt;/p&gt;&#xA;&lt;p&gt;Beyond the technical work, I traveled across Mumbai, Bengaluru, and Pune to study how horticulture crop trading actually happens on the groundâ€”demoing the solution to farmers and traders, and learning firsthand about the challenges of agricultural markets.&lt;/p&gt;&#xA;</description>
    </item>
    <item>
      <title>Sparse AutoEncoders(SAEs) and compression</title>
      <link>http://localhost:1313/posts/saes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/saes/</guid>
      <description>&lt;p&gt;To find important features in A*x=b&#xA;There is another method than directly using Lasso regularization to solve this equation.&lt;/p&gt;&#xA;&lt;p&gt;Thatâ€™s where SAEs come in, Sparse AutoEncoders.&#xA;This is specifically for LLMs.&lt;br&gt;&#xA;So hop on.&lt;/p&gt;&#xA;&lt;p&gt;Lets say I have 4 words in my input, so total size of input is 4* d_model ( lets say 512 in this case).&#xA;And thats lets assume for gemma-2b, so 18 layers in total. ( so 4&lt;em&gt;512&lt;/em&gt;18 total numbers).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Traveling</title>
      <link>http://localhost:1313/misc/travel/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/misc/travel/</guid>
      <description>&lt;p&gt;Have discovered traveling to me means meeting the local people and understanding their culture, and less visiting the touristy places.&#xA;Still discovering what it means to me( I know this much I like adventure sports)&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Places I have been to - ( International)&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;London - oxford, loved seeing the student culture there, all the touristy places&lt;/p&gt;&#xA;&lt;p&gt;Singapore - been here twice , universal studios, botanic gardens ( played with turtle here ðŸ¥°), visited NUS ( befriended a local here and learnt a bit about Singaporeâ€™s culture ), and strolled around gardens by the bay area, won $10 in casino ( probability rate of winning 97% but expected value is -100)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Daily articles (about 150)</title>
      <link>http://localhost:1313/projects/articles/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/projects/articles/</guid>
      <description>Curated and shared the best essays from Pmarca, Paul Graham, Ben Kuhn, Sam Altman, and selected books - delivered weekly over Notion to ~20 friends for 5 months. The experiment tested a hypothesis: that consistently pushing high-quality ideas straight to WhatsApp could shift how people think and reason.</description>
    </item>
    <item>
      <title>Major learnings</title>
      <link>http://localhost:1313/misc/convs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/misc/convs/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;Conversations&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;Appeal to interest, not reason ~ Benjamin Franklin&lt;br&gt;&#xA;I try to use it very often.&lt;br&gt;&#xA;Try to get inside other personâ€™s head and see things from their point of view.&lt;br&gt;&#xA;And try to map, whatever action that other person is doing, at some point of your life, you would have been doing the same thing, so what were you thinking at that time, and how you could rectify that.&lt;br&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>SMs, Cores, Threads, Blocks, SMA, coalesced memory </title>
      <link>http://localhost:1313/posts/gpu_stuff/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/gpu_stuff/</guid>
      <description>&lt;p&gt;When we use GPUs, and code it in CUDA - there are some things which we notice : threads and blocks, and how exactly are the functions running, like all the threads and all the blocks are running simultaneously or consecutively?&lt;/p&gt;&#xA;&lt;p&gt;Start with SMs, and cores.&#xA;So Streaming Multiprocessor ( SM) - aka compute units - SIMD units. SM is like core of multi core CPU, it is a hardware building block&#xA;And Each SM contains a bunch of cores, each core&#xA;( Like Kepler has 192 cores, Maxwell, Pascal, Ampere, Ada has 128 cores, and Turing, A100,A30 has 62 cores)&lt;/p&gt;</description>
    </item>
    <item>
      <title>ELBO losses and Posterior Approximation</title>
      <link>http://localhost:1313/posts/elbolosses/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/elbolosses/</guid>
      <description>&lt;p&gt;The ultimate goal is to maximize the evidence( or the log of the likelihood)&#xA;which is similar as minimizing the negative log likelihood, and hence we compute gradients of the loss( which is negative log likelihood), to move in a direction where loss gets reduced).&#xA;Loss is not a fixed quantity, loss depends on parameters, and hence we can tune the parameters, so that we can get a reduced loss.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mechanistic Interpretability</title>
      <link>http://localhost:1313/projects/c--mech-interp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/projects/c--mech-interp/</guid>
      <description>Studied residual stream activations across layers and explored emergence of dominant directions using refusal features&#xA;in Gemma-2B and Meta LLaMa-7B Model, using both base and instruction fine tuned models</description>
    </item>
    <item>
      <title>Movies</title>
      <link>http://localhost:1313/misc/movies/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/misc/movies/</guid>
      <description>&lt;p&gt;Some of my favs ( pls send reccs):&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Arrival&lt;/li&gt;&#xA;&lt;li&gt;Back to the future&lt;/li&gt;&#xA;&lt;li&gt;3 Idiots&lt;/li&gt;&#xA;&lt;li&gt;Sound of metal&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Books</title>
      <link>http://localhost:1313/misc/books/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/misc/books/</guid>
      <description>&lt;p&gt;Some of my favs :&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Almost everything by Walter Issacson - Steve Jobs, Innovators, Code breakers, Leonardo Di Vinci, Elon Musk, Albert Einstein&lt;/li&gt;&#xA;&lt;li&gt;Physics of the impossible - Michio Kaku&lt;/li&gt;&#xA;&lt;li&gt;Will my cat eat my eyeballs&lt;/li&gt;&#xA;&lt;li&gt;Percy Jackson series&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Will add more&lt;/p&gt;</description>
    </item>
    <item>
      <title>Decentralized Stablecoins</title>
      <link>http://localhost:1313/projects/d---decentralized-stablecoin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/projects/d---decentralized-stablecoin/</guid>
      <description>Borrowers lock up collateral to mint stablecoins, while savers earn interest on their deposits. Adjustable knobs : borrower APR and saver rate ,dynamically balance supply and demand: higher saver rates attract deposits, while lower borrowing rates boost lending. If collateral value falls too far, liquidation kicks in to protect the system.</description>
    </item>
    <item>
      <title>How transformers work, Encoder Decoder difference, How Inputs gets passed</title>
      <link>http://localhost:1313/posts/tranformers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/tranformers/</guid>
      <description>&lt;p&gt;I was confused for a very long time what the actual difference between encoders and decoders are, and how inputs gets processed by each layer( yes, I know the usuals like decoder only has casual mask, and it canâ€™t look ahead, these all sound nice, but I want to know what exactly is happening over there)&lt;/p&gt;&#xA;&lt;p&gt;So, a tldr:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Encoders take embedding of each word, and does some processing on each word and passes it.&#xA;Ex - A 4 sentence word, so 4  - 512 dimensional embedding, after many layers of encoders, we see 4  - 512 dimensional embedding only.&lt;/li&gt;&#xA;&lt;li&gt;Decoder can do 2 things, depending on whether gold output is available or not( inference time vs training time)&#xA;If gold output is available( during training time) - it will similarly take take 4 - 512 dimensional embedding, and after various layers, calculate a final 4 - 512 dimensional vectors. The interesting thing is in decoder, attention is calculated twice, once between the gold output upto that index, and then between the query of the current index, and the keys and value of the final output of the encoder. We can do it all at once by using causal masking.&#xA;If gold output is not available ( during inference time) - it will take only 1 - 512 dimensional vector and further refine it to arrive at one final 512 dimensional vector( it will calculate attention upto the outputs produced upto that index, and then again calculate attention with it being the query matrix, and the keys and value matrix coming from the final encoder layer)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;So, difference is mainly how many vectors get passed at each layer, and how many times attention is being calculated.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Over-Collaterized lending dApp</title>
      <link>http://localhost:1313/projects/over-collaterized-lending-site/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/projects/over-collaterized-lending-site/</guid>
      <description>Over-collateralized lending dApp with two tokens: CORN (collateral) and ETH (borrowable).&#xA;Users deposit CORN as collateral and can borrow ETH against it, but only within a safe limit. If their collateral ratio drops below 110%, the position is flagged for liquidation : protecting the system while keeping lending secure.</description>
    </item>
    <item>
      <title>Understanding ViViT and how Videos are being encoded</title>
      <link>http://localhost:1313/posts/vivit/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/vivit/</guid>
      <description>&lt;p&gt;We have a bunch of videos, so how do we encode it and pass it through a transformer for classification task.&lt;/p&gt;&#xA;&lt;p&gt;Hereâ€™s how:&#xA;Lets say Input video is black and white( so input channels are 1),&#xA;and each frame is 28*28, and there are 28 frames in total( in simple terms, we have each video which is ~3 seconds long)&lt;/p&gt;&#xA;&lt;p&gt;So, hereâ€™s how we convert these videos to a sequence to sequence layer( akin to how ViViT works):&lt;/p&gt;</description>
    </item>
    <item>
      <title>Async Functions</title>
      <link>http://localhost:1313/posts/asyncfunctions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/asyncfunctions/</guid>
      <description>&lt;p&gt;So I had to calls a server over a lakh times to get some output.&lt;br&gt;&#xA;Then had to resort to async functions ( as I already has the list which I need to pass as input)&lt;/p&gt;&#xA;&lt;p&gt;But some observations about how it works â†’&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;async&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;func&lt;/span&gt;()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}  &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;async&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;():&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tasks &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [asyncio&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;create_task(func(i)) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;await&lt;/span&gt; asyncio&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;gather(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;tasks)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;asyncio&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;run(main())&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So what happens is â†’ tasks makes list of all the numbers which need to be executed&#xA;and when asyncio.gather(*tasks) is called, it executes all of them in parallel.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Prediction Market on Ethereum( Sepolia)</title>
      <link>http://localhost:1313/projects/prediction-market/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/projects/prediction-market/</guid>
      <description>&lt;p&gt;Prediction market on Ethereum where people can trade simple Yes/No shares. Prices move with demand, effectively reflecting the crowdâ€™s probability of an outcome, and when the result is known, winners get paid.&lt;/p&gt;&#xA;&lt;p&gt;The core engine runs on an AMM for instant trading, liquidity providers earn a share of fees, and an oracle is used to settle the final result.&lt;/p&gt;&#xA;</description>
    </item>
    <item>
      <title>Stock Market Prediction with RNN</title>
      <link>http://localhost:1313/posts/stocks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/stocks/</guid>
      <description>&lt;p&gt;In trying to predict the closing price of a stock, if we have opening, high, and closing price for several days, I encountered a few questions.&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;How should my input be, and what should my output be?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;â€”&amp;gt;  I knew all the backprop steps in calculating RNN, but I was not able to figure out, what it means to apply sequence to sequence modeling here, how many days should determine the output of the next days( like should a sequence of 7 days determine the output of the 8th day, or lesser or more, or should only 1 day determine the output of next day, and i should calculate my output prediction of all the days, and how should my hidden layer be, do i calculate output at every day, and Hoping to pass hidden state value at every step)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Variational Learning for Hyperelastic Modelilng</title>
      <link>http://localhost:1313/projects/variational-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/projects/variational-learning/</guid>
      <description>Implemented Bayesian Variational Learning algorithm with sparsity-promoting spike-slab priors (to reduce computation)&#xA;to obtain the best-fitting hyper elasticity model from force &amp;amp; displacement data (generatedartificially) using Gibbs Sampling</description>
    </item>
    <item>
      <title>Video-Vision Tranformers</title>
      <link>http://localhost:1313/projects/video-vision-transformers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/projects/video-vision-transformers/</guid>
      <description>Implemented Pytorch version of ViVIT to classify MedMNIST dataset, sampling 3D frames using Tubelet Embedding&#xA;Sampling method, dissecting video into spatial and temporal tokens, and inputting this as embedding to Transformer</description>
    </item>
    <item>
      <title>Signoz for dummies: Sending K8s data to Signoz cloud</title>
      <link>http://localhost:1313/posts/signoz-for-dummies-sending-k8s-data-to-signoz-cloud/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/signoz-for-dummies-sending-k8s-data-to-signoz-cloud/</guid>
      <description>&lt;p&gt;Hello,&#xA;We are going to use Signoz to send Kubernetes data to Signoz cloud, and then create an alert if one of the pods are in pending state.&lt;/p&gt;&#xA;&lt;Br&gt;&#xA;So lets get started :&lt;Br&gt;&#xA;Firstly, &#xA;What is Kubernetes, what is this thing we are doing here, where does Signoz come into the picture, can we do it without Signoz.&#xA;Will answer all of them.&#xA;&lt;p&gt;Kubernetes?&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Kubernetes â€” cluster orchestration for containers, with this you can manage and track a lot of containers across different clusters.&#xA;Where it&amp;rsquo;s useful?&#xA;â€” Not for everyday use cases and that&amp;rsquo;s why a lot of people don&amp;rsquo;t know about it, but in cases where the resources get huge and you have to manage a lot of cluster, like for large applications.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
