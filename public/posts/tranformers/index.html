<!doctype html>







<html
  class="not-ready lg:text-base"
  style="--bg:"
  lang="en"
  dir="ltr"
><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>How transformers work, Encoder Decoder difference, How Inputs gets passed - Shivam Gupta</title>

  
  <meta name="theme-color" />

  <meta name="description" content="I was confused for a very long time what the actual difference between encoders and decoders are, and how inputs gets processed by each layer( yes, I know the usuals like decoder only has casual mask, and it can’t look ahead, these all sound nice, but I want to know what exactly is happening over there)
So, a tldr:

Encoders take embedding of each word, and does some processing on each word and passes it.
Ex - A 4 sentence word, so 4  - 512 dimensional embedding, after many layers of encoders, we see 4  - 512 dimensional embedding only.
Decoder can do 2 things, depending on whether gold output is available or not( inference time vs training time)
If gold output is available( during training time) - it will similarly take take 4 - 512 dimensional embedding, and after various layers, calculate a final 4 - 512 dimensional vectors. The interesting thing is in decoder, attention is calculated twice, once between the gold output upto that index, and then between the query of the current index, and the keys and value of the final output of the encoder. We can do it all at once by using causal masking.
If gold output is not available ( during inference time) - it will take only 1 - 512 dimensional vector and further refine it to arrive at one final 512 dimensional vector( it will calculate attention upto the outputs produced upto that index, and then again calculate attention with it being the query matrix, and the keys and value matrix coming from the final encoder layer)

So, difference is mainly how many vectors get passed at each layer, and how many times attention is being calculated." />
  <meta name="author" content="Shivam Gupta" /><link rel="preload stylesheet" as="style" href="http://localhost:1313/main.min.css" />

  
  <link rel="preload" as="image" href="http://localhost:1313/theme.png" />

  <link rel="preload" as="image" href="https://www.gravatar.com/avatar/a81bbaf54d8b4232f35a22cad694eaa6?s=160&amp;d=identicon" />

  <link rel="preload" as="image" href="http://localhost:1313/twitter.svg" /><link rel="preload" as="image" href="http://localhost:1313/github.svg" />

  <script
    defer
    src="http://localhost:1313/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>

  
  <link
    rel="icon"
    href="http://localhost:1313/favicon.ico"
  />
  <link
    rel="apple-touch-icon"
    href="http://localhost:1313/apple-touch-icon.png"
  />

  <meta name="generator" content="Hugo 0.148.2">
</head>
<body
    class="bg-(--bg) text-black antialiased duration-200 ease-out [-webkit-tap-highlight-color:transparent] dark:text-white"
  ><header
  class="mx-auto flex h-[4.5rem] max-w-(--w) px-8 whitespace-nowrap lg:justify-center"
>
  <div class="relative z-50 flex items-center ltr:mr-auto rtl:ml-auto">
    <a
      class="-translate-y-[1px] text-2xl font-medium"
      href="http://localhost:1313/"
      >Shivam Gupta</a
    >
    <div
      class="btn-dark text-[0px] ltr:ml-4 rtl:mr-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]"
      role="button"
      aria-label="Dark"
    ></div>
  </div>

  <div
    class="btn-menu relative z-50 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden ltr:-mr-8 rtl:-ml-8"
    role="button"
    aria-label="Menu"
  ></div>

  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = ''.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full flex-col justify-center bg-(--bg) pb-16 duration-200 select-none lg:static lg:h-auto lg:flex-row lg:bg-transparent! lg:pb-0 lg:transition-none"
  ><nav
      class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-10 rtl:space-x-reverse"
    ><a
        class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal"
        href="/projects/"
        >Projects</a
      ><a
        class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal"
        href="/posts/"
        >Blog</a
      ><a
        class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal"
        href="/misc/"
        >Misc</a
      ></nav><nav
      class="mt-12 flex justify-center space-x-10 lg:mt-0 lg:items-center ltr:lg:ml-14 rtl:space-x-reverse rtl:lg:mr-14 dark:invert"
    >
      <a
        class="h-7 w-7 text-[0px] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./twitter.svg)"
        href="https://twitter.com/https://twitter.com/reversemagnus"
        target="_blank"
        rel="me"
      >twitter</a>
      <a
        class="h-7 w-7 text-[0px] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./github.svg)"
        href="https://github.com/https://github.com/revmag"
        target="_blank"
        rel="me"
      >github</a>
    </nav>
  </div>
</header>
<main
      class="prose prose-neutral dark:prose-invert relative mx-auto min-h-[calc(100vh-9rem)] max-w-(--w) px-8 pt-14 pb-16"
    ><article>
  <header class="mb-14">
    <h1 class="my-0! pb-2.5">How transformers work, Encoder Decoder difference, How Inputs gets passed</h1><div class="text-xs antialiased opacity-60"></div></header>

  <section><p>I was confused for a very long time what the actual difference between encoders and decoders are, and how inputs gets processed by each layer( yes, I know the usuals like decoder only has casual mask, and it can’t look ahead, these all sound nice, but I want to know what exactly is happening over there)</p>
<p>So, a tldr:</p>
<ol>
<li>Encoders take embedding of each word, and does some processing on each word and passes it.
Ex - A 4 sentence word, so 4  - 512 dimensional embedding, after many layers of encoders, we see 4  - 512 dimensional embedding only.</li>
<li>Decoder can do 2 things, depending on whether gold output is available or not( inference time vs training time)
If gold output is available( during training time) - it will similarly take take 4 - 512 dimensional embedding, and after various layers, calculate a final 4 - 512 dimensional vectors. The interesting thing is in decoder, attention is calculated twice, once between the gold output upto that index, and then between the query of the current index, and the keys and value of the final output of the encoder. We can do it all at once by using causal masking.
If gold output is not available ( during inference time) - it will take only 1 - 512 dimensional vector and further refine it to arrive at one final 512 dimensional vector( it will calculate attention upto the outputs produced upto that index, and then again calculate attention with it being the query matrix, and the keys and value matrix coming from the final encoder layer)</li>
</ol>
<p>So, difference is mainly how many vectors get passed at each layer, and how many times attention is being calculated.</p>
<p>And after this, the final 512 dimensional  vector is feed forwaded to tgt_vocab_size, to produce score for each word, and then softmax taken, and various sampling schemes can be applied here, like greedy, beam search, to get the desired result.</p>
<p>Also interesting is words really hold no significance, we can really establish some really beautiful results with a bunch of numbers.</p>
</section>

  </article></main><footer
  class="mx-auto flex h-[4.5rem] max-w-(--w) items-center px-8 text-xs tracking-wider uppercase opacity-60"
>
  <div class="mr-auto">© Shivam Gupta</div>
  <a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank"
    >powered by hugo️️</a
  >️
  <a
    class="link"
    href="https://github.com/nanxiaobei/hugo-paper"
    rel="noopener"
    target="_blank"
    >hugo-paper</a
  >
</footer>
</body>
</html>
