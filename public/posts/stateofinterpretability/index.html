<!doctype html>







<html
  class="not-ready lg:text-base"
  style="--bg:"
  lang="en"
  dir="ltr"
><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>State of Mechanistic Interpretability - Shivam Gupta</title>

  
  <meta name="theme-color" />

  <meta name="description" content="So I deep dived into what is going in the field of mechanistic interpretability( it seems really cool, to be able to know how attention is calculating all these amazing similarities from so far, and how it is able to make so much sense).
What I think is happening -
It seems to me, LLMs mimic a lazy person, who is very good at pattern patching, so it learns rules/pattern of some kind implicitly, and its not entirely perfect knowledge like code, but it wants to converge to that. I got this idea by looking at the chess paper, where they analyzed how it predicts the next move of chess game, without being fed the rules explicitly, or what each piece does, but after training for several games, it was able to predict the next move, and also tell some of the rules, Some for calculation, there is this paper, which analyzed how LLMs are able to do 5-6 digit addition, it was amazing, it tried to show they are learning the rules of addition somehow( like addition at ones place, and then carry on if sum exceeds 10), by differentiating the dataset into different catagories (like whether carry sum is needed or not)." />
  <meta name="author" content="Shivam Gupta" /><link rel="preload stylesheet" as="style" href="http://localhost:1313/main.min.css" />

  
  <link rel="preload" as="image" href="http://localhost:1313/theme.png" />

  <link rel="preload" as="image" href="https://www.gravatar.com/avatar/a81bbaf54d8b4232f35a22cad694eaa6?s=160&amp;d=identicon" />

  <link rel="preload" as="image" href="http://localhost:1313/twitter.svg" /><link rel="preload" as="image" href="http://localhost:1313/github.svg" />

  <script
    defer
    src="http://localhost:1313/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>

  
  <link
    rel="icon"
    href="http://localhost:1313/favicon.ico"
  />
  <link
    rel="apple-touch-icon"
    href="http://localhost:1313/apple-touch-icon.png"
  />

  <meta name="generator" content="Hugo 0.148.2">
</head>
<body
    class="bg-(--bg) text-black antialiased duration-200 ease-out [-webkit-tap-highlight-color:transparent] dark:text-white"
  ><header
  class="mx-auto flex h-[4.5rem] max-w-(--w) px-8 whitespace-nowrap lg:justify-center"
>
  <div class="relative z-50 flex items-center ltr:mr-auto rtl:ml-auto">
    <a
      class="-translate-y-[1px] text-2xl font-medium"
      href="http://localhost:1313/"
      >Shivam Gupta</a
    >
    <div
      class="btn-dark text-[0px] ltr:ml-4 rtl:mr-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]"
      role="button"
      aria-label="Dark"
    ></div>
  </div>

  <div
    class="btn-menu relative z-50 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden ltr:-mr-8 rtl:-ml-8"
    role="button"
    aria-label="Menu"
  ></div>

  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = ''.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full flex-col justify-center bg-(--bg) pb-16 duration-200 select-none lg:static lg:h-auto lg:flex-row lg:bg-transparent! lg:pb-0 lg:transition-none"
  ><nav
      class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-10 rtl:space-x-reverse"
    ><a
        class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal"
        href="/projects/"
        >Projects</a
      ><a
        class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal"
        href="/posts/"
        >Blog</a
      ><a
        class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal"
        href="/misc/"
        >Misc</a
      ></nav><nav
      class="mt-12 flex justify-center space-x-10 lg:mt-0 lg:items-center ltr:lg:ml-14 rtl:space-x-reverse rtl:lg:mr-14 dark:invert"
    >
      <a
        class="h-7 w-7 text-[0px] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./twitter.svg)"
        href="https://twitter.com/https://twitter.com/reversemagnus"
        target="_blank"
        rel="me"
      >twitter</a>
      <a
        class="h-7 w-7 text-[0px] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./github.svg)"
        href="https://github.com/https://github.com/revmag"
        target="_blank"
        rel="me"
      >github</a>
    </nav>
  </div>
</header>
<main
      class="prose prose-neutral dark:prose-invert relative mx-auto min-h-[calc(100vh-9rem)] max-w-(--w) px-8 pt-14 pb-16"
    ><article>
  <header class="mb-14">
    <h1 class="my-0! pb-2.5">State of Mechanistic Interpretability</h1><div class="text-xs antialiased opacity-60"></div></header>

  <section><p>So I deep dived into what is going in the field of mechanistic interpretability( it seems really cool, to be able to know how attention is calculating all these amazing similarities from so far, and how it is able to make so much sense).</p>
<p>What I think is happening -
It seems to me, LLMs mimic a lazy person, who is very good at pattern patching, so it learns rules/pattern of some kind implicitly, and its not entirely perfect knowledge like code, but it wants to converge to that. I got this idea by looking at the chess paper, where they analyzed how it predicts the next move of chess game, without being fed the rules explicitly, or what each piece does, but after training for several games, it was able to predict the next move, and also tell some of the rules, Some for calculation, there is this paper, which analyzed how LLMs are able to do 5-6 digit addition, it was amazing, it tried to show they are learning the rules of addition somehow( like addition at ones place, and then carry on if sum exceeds 10), by differentiating the dataset into different catagories (like whether carry sum is needed or not).</p>
<pre tabindex="0"><code>`So its a pattern matching algo, which implicitly learns the privileged basis for the problem at hand and tries to solve similar things`
</code></pre><aside>
💡 Goal is to find the basis function of any task, and use that to train the model ( I saw a post where someone as OpenAI described their day to day job, it was to solve puzzles, and then tell model how they solved it, explicitly)
</aside>
<p>Here’s an overview of what I read-</p>
<ol>
<li>There are various methods being used, and different methods work for different cases. Some things which are quite accepted now are -
Induction heads- only found in 2L models,
Features being present which tell the output came to be( and we can insert different features in different results, PS -Hello Golden Gate Claude) ,
Zero ablation of heads - we make the attention score of heads 0 and see whether this significantly changes the output or not,
LogitLens - this is nothing new - but it is remarkable in the sense that we have some idea that transformers are able to get back at identity after some layers
TranformerLens - we study each individual head of a toy model - and try to find any relations between the different heads, induction head, prev token head came from this</li>
</ol>
<p>And there is a whole lot of interesting things happening like
Catastrophic forgetting - when model is fine tuned on new data, it seems to forget some of the old data
The methods for diving deep are causal  tracing, ablation, looking at KV cache, looking at weights, feature weights, steering weights, existence of privileged basis vs superposition basis.
Double descent( grokking), when model performance improves, gets bad and then improves again with the increment of training data - it was good till the point improving and getting bad, as this explains overfitting on training data, but then again it suddenly getting better.
( The graph below)</p>
<div class="home-hero">
  <img src="/images/interpretability.png" alt="Me" width="1800">
  <div class="home-quote">
</div>
</div>
<p>Then there are some cases of whether the LLM is actually just memorizing things from its training data( a paper showed it gave input the first sentence of harry potter, and it produced the entire page verbatim)</p>
<p>Some further questions like are more layers better( seems true with the analysis of LogitLens, at the final layers, the model somehow seems to be getting idea of what the actual output can be)
Whether phase change is inherent part of how models learn?</p>
<p>And further analyse Edge cases where
linearising LayerNorm breaks, activation patching breaks, causal scrubbing breaks, ablation breaks, composition scores break, eigenvalue copying score break</p>
<p>Most of the research is doing inference on models and see what the weights are, maybe doing fine tuning on these things can give us further answers, and inserting our own feature vectors( Hello, Claude’s Golden Gate bridge)</p>
<p>Links:</p>
<ol>
<li><a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens">https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens</a></li>
<li><a href="https://adamkarvonen.github.io/machine_learning/2024/01/03/chess-world-models.html">https://adamkarvonen.github.io/machine_learning/2024/01/03/chess-world-models.html</a></li>
<li><a href="https://arxiv.org/abs/2310.15213">https://arxiv.org/abs/2310.15213</a> - function vectors</li>
<li><a href="https://www.alignmentforum.org/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction">https://www.alignmentforum.org/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction</a> - refusal</li>
<li><a href="https://philipquirke.github.io/transformer-maths/2023/10/14/Understanding-Addition.html">https://philipquirke.github.io/transformer-maths/2023/10/14/Understanding-Addition.html</a> - this explains addition by privileged basis</li>
<li>I got many of my ideas and some possible research directions by looking at the problems people are trying to solve( from 200 open problems in Mech Interpretability)  - <a href="https://docs.google.com/spreadsheets/d/1oOdrQ80jDK-aGn-EVdDt3dg65GhmzrvBWzJ6MUZB8n4/edit#gid=0">https://docs.google.com/spreadsheets/d/1oOdrQ80jDK-aGn-EVdDt3dg65GhmzrvBWzJ6MUZB8n4/edit#gid=0</a></li>
<li><a href="https://colab.research.google.com/drive/1NYjR3tjOiDJ2v8nv3mhrph-_IM4p9goS?usp=sharing-">https://colab.research.google.com/drive/1NYjR3tjOiDJ2v8nv3mhrph-_IM4p9goS?usp=sharing-</a> was doing the exercises in Neel Nanda’s guide to mech interpretability on Function vectors and model steering</li>
</ol>
</section>

  <nav
    class="mt-24 flex overflow-hidden rounded-xl bg-black/[3%] text-lg leading-[1.2]! *:flex *:w-1/2 *:items-center *:p-5 *:font-medium *:no-underline dark:bg-white/[8%] [&>*:hover]:bg-black/[2%] dark:[&>*:hover]:bg-white/[3%]"
  ><a
      class="justify-end pl-3 ltr:ml-auto rtl:mr-auto"
      href="http://localhost:1313/posts/saes/"
      ><span>Sparse AutoEncoders(SAEs) and compression</span><span class="ltr:ml-1.5 rtl:mr-1.5">→</span></a
    ></nav></article></main><footer
  class="mx-auto flex h-[4.5rem] max-w-(--w) items-center px-8 text-xs tracking-wider uppercase opacity-60"
>
  <div class="mr-auto">© Shivam Gupta</div>
  <a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank"
    >powered by hugo️️</a
  >️
  <a
    class="link"
    href="https://github.com/nanxiaobei/hugo-paper"
    rel="noopener"
    target="_blank"
    >hugo-paper</a
  >
</footer>
</body>
</html>
